{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaffizy/Sentimental-Analysist/blob/main/Shaffi's_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYvfNom6Wdbr"
      },
      "source": [
        "**Importing the Dependencies**\n",
        "In this code, I imported the following dependencies: Pandas, Scikit-learn, Tensorflow.\n",
        "Pandas for to visualize the data in this this programming environment\n",
        "Scikit-Learn to divide the data to test and train data.\n",
        "Tensorflow for the training of the date when it is split to train and test.We would use the Dense, Embedding and LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C1TTsclJH4vX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKN8lVqoYr3I"
      },
      "source": [
        "**Data Collection-kaggle API**\n",
        "This function is used to load the file \"kaggle.json\" which contains my kaggle information (name, password) to enable the download of a particular kaggle file called \"imdb-dataset-of-50k-movie-reviews\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GasI5x68I7Jl"
      },
      "outputs": [],
      "source": [
        "def Data_Collection():\n",
        "    kaggle_dictionary = json.load(open(\"kaggle.json\"))\n",
        "    os.environ[\"KAGGLE_USERNAME\"] = kaggle_dictionary[\"username\"]\n",
        "    os.environ[\"KAGGLE_KEY\"] = kaggle_dictionary[\"key\"]\n",
        "Data_Collection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NtBtnkiSI0dY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d27ba8ba-4949-477a-eeba-97fa58848745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
            "License(s): other\n",
            "Downloading imdb-dataset-of-50k-movie-reviews.zip to /content\n",
            " 97% 25.0M/25.7M [00:01<00:00, 26.6MB/s]\n",
            "100% 25.7M/25.7M [00:01<00:00, 16.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3hx2Tp3bL64"
      },
      "source": [
        "**Data Extraction**\n",
        "This function is used to unzip the \"imdb-dataset-of-50k-movie-reviews\" so it can be visualized by pandas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8rXe3DDkIAM0"
      },
      "outputs": [],
      "source": [
        "def Data_Extraction():\n",
        "  with ZipFile(\"imdb-dataset-of-50k-movie-reviews.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "Data_Extraction()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8oLUCbGc1Sb"
      },
      "source": [
        "**Data Processing**\n",
        "In this function I used pandas to read through the data.Then I replaced the \"positive\" and \"negative\" value in the \"sentiment\" column of the *data* that is the variable of the result of the it's visualization by pandas to integer so the machine can read and understand. After that I defined the *train* and *test* data by spliting them with scikit-learn. Tokenization is now carried out to replace the word with corresponding numbers so the machine can easily work with it. While X(train, test) and Y(train, test) are then initialized with the former being the data of values of \"review\" and the latter beign the data of values of \"sentiment\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CO7dUIKBa5Zg"
      },
      "outputs": [],
      "source": [
        "def Data_Processing():\n",
        "    data = pd.read_csv(\"/content/IMDB Dataset.csv\")\n",
        "    data.replace({\"sentiment\": {\"positive\": 1, \"negative\": 0}}, inplace=True)\n",
        "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "    print(train_data.shape)\n",
        "    print(test_data.shape)\n",
        "\n",
        "    return train_data , test_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the values of train and test data\n",
        "train_data , test_data = Data_Processing()\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(train_data[\"review\"])\n",
        "\n",
        "X_train = pad_sequences(tokenizer.texts_to_sequences(train_data[\"review\"]), maxlen=200)\n",
        "X_test = pad_sequences(tokenizer.texts_to_sequences(test_data[\"review\"]), maxlen=200)\n",
        "print(X_train)\n",
        "print(X_test)\n",
        "\n",
        "Y_train = train_data[\"sentiment\"]\n",
        "Y_test = test_data[\"sentiment\"]\n",
        "print(Y_train)\n",
        "print(Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm4vOYKW6AXf",
        "outputId": "9e800ebd-3122-483a-97d8-d6f506ba33e1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40000, 2)\n",
            "(10000, 2)\n",
            "[[1935    1 1200 ...  205  351 3856]\n",
            " [   3 1651  595 ...   89  103    9]\n",
            " [   0    0    0 ...    2  710   62]\n",
            " ...\n",
            " [   0    0    0 ... 1641    2  603]\n",
            " [   0    0    0 ...  245  103  125]\n",
            " [   0    0    0 ...   70   73 2062]]\n",
            "[[   0    0    0 ...  995  719  155]\n",
            " [  12  162   59 ...  380    7    7]\n",
            " [   0    0    0 ...   50 1088   96]\n",
            " ...\n",
            " [   0    0    0 ...  125  200 3241]\n",
            " [   0    0    0 ... 1066    1 2305]\n",
            " [   0    0    0 ...    1  332   27]]\n",
            "39087    0\n",
            "30893    0\n",
            "45278    1\n",
            "16398    0\n",
            "13653    0\n",
            "        ..\n",
            "11284    1\n",
            "44732    1\n",
            "38158    0\n",
            "860      1\n",
            "15795    1\n",
            "Name: sentiment, Length: 40000, dtype: int64\n",
            "33553    1\n",
            "9427     1\n",
            "199      0\n",
            "12447    1\n",
            "39489    0\n",
            "        ..\n",
            "28567    0\n",
            "25079    1\n",
            "18707    1\n",
            "15200    0\n",
            "5857     1\n",
            "Name: sentiment, Length: 10000, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoQMFZ4qkLaj"
      },
      "source": [
        "**LSTM - Long Short-Term Memory**\n",
        "I used this function to build the model with Embedding, LSTM, Dense provided by tensorflow and then I compile the model together so everything works simultaneously in concurrent order and I train the model the finally carry out model evaluation which shows the accuracy of the machine  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8PK_yudjbBxU"
      },
      "outputs": [],
      "source": [
        "def Trainning():\n",
        "    # build the model\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\n",
        "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "    model.summary()\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "model = Trainning()\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# training the Model\n",
        "model.fit(X_train, Y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# model Evaluation\n",
        "loss, accuracy = model.evaluate(X_test, Y_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "1c-daZhe7npl",
        "outputId": "ecfecace-9984-4ea1-d046-0e5431317225"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 480ms/step - accuracy: 0.7300 - loss: 0.5215 - val_accuracy: 0.7596 - val_loss: 0.4861\n",
            "Epoch 2/5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 445ms/step - accuracy: 0.8334 - loss: 0.3829 - val_accuracy: 0.8026 - val_loss: 0.4219\n",
            "Epoch 3/5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 450ms/step - accuracy: 0.8638 - loss: 0.3340 - val_accuracy: 0.8655 - val_loss: 0.3295\n",
            "Epoch 4/5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 464ms/step - accuracy: 0.8899 - loss: 0.2752 - val_accuracy: 0.8676 - val_loss: 0.3352\n",
            "Epoch 5/5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 443ms/step - accuracy: 0.9054 - loss: 0.2419 - val_accuracy: 0.8745 - val_loss: 0.3194\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 103ms/step - accuracy: 0.8789 - loss: 0.3037\n",
            "Test Loss: 0.3009601831436157\n",
            "Test Accuracy: 0.8820000290870667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75vg_3uWmbkd"
      },
      "source": [
        "**Building a Predictive System** Testing and verification of what the machine has learnt would happen here. I would test various test case with original reviews.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Aui4il8BmSQF"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(review):\n",
        "\n",
        "  # tokenize and pad the review\n",
        "  sequence = tokenizer.texts_to_sequences([review])\n",
        "  padded_sequence = pad_sequences(sequence, maxlen=200)\n",
        "  prediction = model.predict(padded_sequence)\n",
        "  sentiment = \"positive\" if prediction[0][0] > 0.5 else \"negative\"\n",
        "\n",
        "  return sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTifz21jyrq8",
        "outputId": "814d4bf6-417f-4d94-cba0-13aee120289b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 281ms/step\n",
            "The sentiment of the review is: positive\n"
          ]
        }
      ],
      "source": [
        "# example usage\n",
        "new_review = \"This movie was fantastic. I loved it.\"\n",
        "sentiment = predict_sentiment(new_review)\n",
        "print(f\"The sentiment of the review is: {sentiment}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIXVAGaivPrMi854SXFcrO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}