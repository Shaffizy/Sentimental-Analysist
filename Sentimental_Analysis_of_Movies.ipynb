{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaffizy/Sentimental-Analysist/blob/main/Sentimental_Analysis_of_Movies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SENTIMENTAL ANALYSIS**\n",
        "Sentiment analysis of movie reviews is a natural language processing (NLP) technique that involves analyzing and interpreting the sentiments expressed in written movie reviews. The primary goal is to determine whether a review conveys a positive, negative, or neutral sentiment about a movie. This process typically involves several steps, including data preprocessing, feature extraction, and model training.\n",
        "\n",
        "How It Works:\n",
        "Data Collection: The process begins by gathering a large dataset of movie reviews, often from online platforms like IMDb, Rotten Tomatoes, or specific datasets provided for sentiment analysis tasks. Each review is typically labeled with its corresponding sentiment (e.g., positive or negative).\n",
        "\n",
        "Data Preprocessing: Before analysis, the text data is cleaned and prepared. This step may involve removing stopwords (common words like \"the\" or \"and\"), stemming or lemmatizing words to their root forms, and converting the text into a format that a machine learning model can understand.\n",
        "\n",
        "Feature Extraction: The next step is to convert the raw text into numerical features. This can be done using techniques like Bag of Words, Term Frequency-Inverse Document Frequency (TF-IDF), or word embeddings (e.g., Word2Vec, GloVe). These features represent the text in a way that a machine learning model can process.\n",
        "\n",
        "Model Training: A machine learning model, such as a logistic regression classifier, support vector machine (SVM), or a neural network, is trained on the labeled data. The model learns to associate certain words, phrases, or patterns in the text with positive or negative sentiments.\n",
        "\n",
        "Prediction and Evaluation: Once the model is trained, it can be used to predict the sentiment of new, unseen movie reviews. The performance of the model is typically evaluated using metrics such as accuracy, precision, recall, and F1-score. More detailed evaluations might include analyzing the model’s performance using a confusion matrix, which provides insight into how well the model distinguishes between positive and negative reviews."
      ],
      "metadata": {
        "id": "0KQFMDWoqIIZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYvfNom6Wdbr"
      },
      "source": [
        "**Importing the Dependencies**\n",
        "In this code, I imported the following dependencies: Pandas, Scikit-learn, Tensorflow.\n",
        "Pandas for to visualize the data in this this programming environment\n",
        "Scikit-Learn to divide the data to test and train data.\n",
        "Tensorflow for the training of the date when it is split to train and test.We would use the Dense, Embedding and LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1TTsclJH4vX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKN8lVqoYr3I"
      },
      "source": [
        "**Data Collection-kaggle API**\n",
        "This function is used to load the file \"kaggle.json\" which contains my kaggle information (name, password) to enable the download of a particular kaggle file called \"imdb-dataset-of-50k-movie-reviews\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GasI5x68I7Jl"
      },
      "outputs": [],
      "source": [
        "def Data_Collection():\n",
        "    kaggle_dictionary = json.load(open(\"kaggle.json\"))\n",
        "    os.environ[\"KAGGLE_USERNAME\"] = kaggle_dictionary[\"username\"]\n",
        "    os.environ[\"KAGGLE_KEY\"] = kaggle_dictionary[\"key\"]\n",
        "Data_Collection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtBtnkiSI0dY",
        "outputId": "5ca07dd7-d535-4477-bd5f-c0888e50a71f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
            "License(s): other\n",
            "imdb-dataset-of-50k-movie-reviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3hx2Tp3bL64"
      },
      "source": [
        "**Data Extraction**\n",
        "This function is used to unzip the \"imdb-dataset-of-50k-movie-reviews\" so it can be visualized by pandas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rXe3DDkIAM0"
      },
      "outputs": [],
      "source": [
        "def Data_Extraction():\n",
        "  with ZipFile(\"imdb-dataset-of-50k-movie-reviews.zip\", \"r\") as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "Data_Extraction()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8oLUCbGc1Sb"
      },
      "source": [
        "**Data Processing**\n",
        "In this function I used pandas to read through the data.Then I replaced the \"positive\" and \"negative\" value in the \"sentiment\" column of the *data* that is the variable of the result of the it's visualization by pandas to integer so the machine can read and understand. After that I defined the *train* and *test* data by spliting them with scikit-learn. Tokenization is now carried out to replace the word with corresponding numbers so the machine can easily work with it. While X(train, test) and Y(train, test) are then initialized with the former being the data of values of \"review\" and the latter beign the data of values of \"sentiment\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO7dUIKBa5Zg"
      },
      "outputs": [],
      "source": [
        "def Data_Processing():\n",
        "    data = pd.read_csv(\"/content/IMDB Dataset.csv\")\n",
        "    data.replace({\"sentiment\": {\"positive\": 1, \"negative\": 0}}, inplace=True)\n",
        "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=70)\n",
        "    print(train_data.shape)\n",
        "    print(test_data.shape)\n",
        "\n",
        "    return train_data , test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm4vOYKW6AXf",
        "outputId": "ad73be58-09a8-4385-903e-1f3b9d749100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40000, 2)\n",
            "(10000, 2)\n",
            "[[   0    0    0 ...   78  174  204]\n",
            " [   0    0    0 ...  377   11   19]\n",
            " [   0    0    0 ...   47   56  527]\n",
            " ...\n",
            " [   0    0    0 ...  112  509  315]\n",
            " [   0    0    0 ...    2  548  346]\n",
            " [   0    0    0 ... 1469   38 2260]]\n",
            "[[ 154    4  259 ... 2451   71   12]\n",
            " [   0    0    0 ...  148    5   68]\n",
            " [   0    0    0 ...   21  276  146]\n",
            " ...\n",
            " [ 292    2   16 ...   76 1879  644]\n",
            " [   0    0    0 ...    2  353 1076]\n",
            " [   7 1829  266 ...   49  683    5]]\n",
            "22375    0\n",
            "43545    1\n",
            "41827    1\n",
            "41018    1\n",
            "46867    1\n",
            "        ..\n",
            "21563    1\n",
            "25916    1\n",
            "44824    0\n",
            "21618    0\n",
            "23886    0\n",
            "Name: sentiment, Length: 40000, dtype: int64\n",
            "14397    1\n",
            "2775     1\n",
            "28441    0\n",
            "7544     0\n",
            "13908    0\n",
            "        ..\n",
            "40441    0\n",
            "13069    0\n",
            "46889    1\n",
            "34084    1\n",
            "47718    0\n",
            "Name: sentiment, Length: 10000, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Initialize the values of train and test data\n",
        "train_data , test_data = Data_Processing()\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(train_data[\"review\"])\n",
        "\n",
        "X_train = pad_sequences(tokenizer.texts_to_sequences(train_data[\"review\"]), maxlen=200)\n",
        "X_test = pad_sequences(tokenizer.texts_to_sequences(test_data[\"review\"]), maxlen=200)\n",
        "print(X_train)\n",
        "print(X_test)\n",
        "\n",
        "Y_train = train_data[\"sentiment\"]\n",
        "Y_test = test_data[\"sentiment\"]\n",
        "print(Y_train)\n",
        "print(Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoQMFZ4qkLaj"
      },
      "source": [
        "**LSTM - Long Short-Term Memory**\n",
        "I used this function to build the model with Embedding, LSTM, Dense provided by tensorflow and then I compile the model together so everything works simultaneously in concurrent order and I train the model the finally carry out model evaluation which shows the accuracy of the machine  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PK_yudjbBxU"
      },
      "outputs": [],
      "source": [
        "def Trainning():\n",
        "    # build the model\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\n",
        "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "    model.summary()\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "1c-daZhe7npl",
        "outputId": "b2fb90c0-7284-4239-9092-fe37d6f78472"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 610ms/step - accuracy: 0.7224 - loss: 0.5352 - val_accuracy: 0.5726 - val_loss: 0.6530\n",
            "Epoch 2/8\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 560ms/step - accuracy: 0.7496 - loss: 0.4991 - val_accuracy: 0.8331 - val_loss: 0.3822\n",
            "Epoch 3/8\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 540ms/step - accuracy: 0.8516 - loss: 0.3506 - val_accuracy: 0.8559 - val_loss: 0.3568\n",
            "Epoch 4/8\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 579ms/step - accuracy: 0.8844 - loss: 0.2886 - val_accuracy: 0.8764 - val_loss: 0.3116\n",
            "Epoch 5/8\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 531ms/step - accuracy: 0.9012 - loss: 0.2505 - val_accuracy: 0.8700 - val_loss: 0.3254\n",
            "Epoch 6/8\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 530ms/step - accuracy: 0.9166 - loss: 0.2169 - val_accuracy: 0.8759 - val_loss: 0.3079\n",
            "Epoch 7/8\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 556ms/step - accuracy: 0.9282 - loss: 0.1873 - val_accuracy: 0.8752 - val_loss: 0.3610\n",
            "Epoch 8/8\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 541ms/step - accuracy: 0.9358 - loss: 0.1713 - val_accuracy: 0.8829 - val_loss: 0.3259\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 122ms/step - accuracy: 0.8807 - loss: 0.3300\n",
            "Test Loss: 0.33072537183761597\n",
            "Test Accuracy: 0.8784999847412109\n"
          ]
        }
      ],
      "source": [
        "# compile the model\n",
        "model = Trainning()\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# training the Model\n",
        "model.fit(X_train, Y_train, epochs=8, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# model Evaluation\n",
        "loss, accuracy = model.evaluate(X_test, Y_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75vg_3uWmbkd"
      },
      "source": [
        "**Building a Predictive System** Testing and verification of what the machine has learnt would happen here. I would test various test case with original reviews.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3Cd5In5knXK",
        "outputId": "ccb8b7e7-389f-48d3-dbe6-310760b2822e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 126ms/step\n",
            "Confusion Matrix:\n",
            " [[4345  638]\n",
            " [ 577 4440]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.87      0.88      4983\n",
            "           1       0.87      0.88      0.88      5017\n",
            "\n",
            "    accuracy                           0.88     10000\n",
            "   macro avg       0.88      0.88      0.88     10000\n",
            "weighted avg       0.88      0.88      0.88     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Predict on the test data\n",
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "# For binary classification, convert predictions to class labels\n",
        "Y_pred_classes = (Y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(Y_test, Y_pred_classes)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Optionally, get a full classification report\n",
        "print(\"Classification Report:\\n\", classification_report(Y_test, Y_pred_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aui4il8BmSQF"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(review):\n",
        "  # tokenize and pad the review\n",
        "  sequence = tokenizer.texts_to_sequences([review])\n",
        "  padded_sequence = pad_sequences(sequence, maxlen=200)\n",
        "  prediction = model.predict(padded_sequence)\n",
        "  sentiment = \"positive\" if prediction[0][0] > 0.5 else \"negative\"\n",
        "  return sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTifz21jyrq8",
        "outputId": "fb2e8f2f-6e80-4142-9854-705df93740e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "The sentiment of the review is: negative\n"
          ]
        }
      ],
      "source": [
        "# example usage\n",
        "new_review = \"what exactly is this movie about, I cant make sense of it\"\n",
        "sentiment = predict_sentiment(new_review)\n",
        "print(f\"The sentiment of the review is: {sentiment}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1rAghajDqIF",
        "outputId": "caf80824-9a78-455d-818a-6793b7e9c222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
            "The sentiment of the review is: negative\n"
          ]
        }
      ],
      "source": [
        "# example usage\n",
        "new_review = \"This movie was not that good\"\n",
        "sentiment = predict_sentiment(new_review)\n",
        "print(f\"The sentiment of the review is: {sentiment}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvL0iO9iD_oq",
        "outputId": "97cb3e91-d91c-4f1d-efa3-9da3eb643eb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "The sentiment of the review is: negative\n"
          ]
        }
      ],
      "source": [
        "# example usage\n",
        "new_review = \"This movie was bad\"\n",
        "sentiment = predict_sentiment(new_review)\n",
        "print(f\"The sentiment of the review is: {sentiment}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObdl8QSJCXvqzngpiVgs3f",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}